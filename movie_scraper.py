import requests
import json
import re
import time
from bs4 import BeautifulSoup

# ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø ‡¶Æ‡ßÅ‡¶≠‡¶ø ‡¶™‡ßá‡¶ú‡ßá‡¶∞ ‡¶≠‡ßá‡¶§‡¶∞‡ßá‡¶∞ ‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶ï‡ßã‡¶° ‡¶∏‡ßç‡¶ï‡ßç‡¶Ø‡¶æ‡¶® ‡¶ï‡¶∞‡¶¨
TARGET_SITES = [
    "https://www.watch-movies.com.pk",
    "https://www.movi.pk"
]

# ‡¶è‡¶ï‡¶¶‡¶Æ ‡¶∞‡¶ø‡ßü‡¶æ‡¶≤ ‡¶¨‡ßç‡¶∞‡¶æ‡¶â‡¶ú‡¶æ‡¶∞ ‡¶π‡ßá‡¶°‡¶æ‡¶∞
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1"
}

def deep_search_links(html):
    """‡¶è‡¶ü‡¶ø ‡¶ú‡¶æ‡¶≠‡¶æ‡¶∏‡ßç‡¶ï‡ßç‡¶∞‡¶ø‡¶™‡ßç‡¶ü ‡¶ï‡ßã‡¶° ‡¶è‡¶¨‡¶Ç ‡¶π‡¶ø‡¶°‡ßá‡¶® ‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó‡ßá‡¶∞ ‡¶≠‡ßá‡¶§‡¶∞ ‡¶•‡ßá‡¶ï‡ßá ‡¶™‡ßç‡¶≤‡ßá‡ßü‡¶æ‡¶∞ ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶¨‡ßá"""
    found = set()
    # ‡ßß. ‡¶ú‡¶®‡¶™‡ßç‡¶∞‡¶ø‡ßü ‡¶≠‡¶ø‡¶°‡¶ø‡¶ì ‡¶π‡ßã‡¶∏‡ßç‡¶ü‡¶ø‡¶Ç ‡¶™‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶∞‡ßç‡¶®
    patterns = [
        r'https?://(?:dood|doodstream|ds2play)\.[a-z0-9]+/e/[a-zA-Z0-9]+',
        r'https?://(?:streamwish|awish|strwish)\.[a-z0-9]+/e/[a-zA-Z0-9]+',
        r'https?://(?:voe|voe-sx)\.[a-z0-9]+/e/[a-zA-Z0-9]+',
        r'https?://(?:streamtape|stape)\.[a-z0-9]+/e/[a-zA-Z0-9]+',
        r'https?://(?:gdriveplayer|gembed)\.[a-z0-9]+/embed\?[^"\']+',
        r'https?://[^\s"\']+\.m3u8[^\s"\']*'
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, html, re.IGNORECASE)
        for m in matches:
            found.add(m.replace('\\', ''))
            
    return found

def main():
    MOVIE_NAMES = ["Deva", "Dhurandhar", "Avatar"]
    final_data = []

    for movie in MOVIE_NAMES:
        print(f"--- üîé Searching: {movie} ---")
        movie_links = set()

        for site in TARGET_SITES:
            try:
                # ‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø ‡¶∏‡¶æ‡¶∞‡ßç‡¶ö ‡¶ï‡ßÅ‡ßü‡ßá‡¶∞‡¶ø ‡¶™‡¶æ‡¶†‡¶æ‡¶®‡ßã
                search_url = f"{site}/?s={movie.replace(' ', '+')}"
                session = requests.Session() # ‡¶∏‡ßá‡¶∂‡¶® ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá ‡¶Ø‡¶æ‡¶§‡ßá ‡¶ï‡ßÅ‡¶ï‡¶ø ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá
                r = session.get(search_url, headers=HEADERS, timeout=20)
                
                soup = BeautifulSoup(r.text, 'html.parser')
                
                # ‡¶Æ‡ßÅ‡¶≠‡¶ø ‡¶™‡ßã‡¶∏‡ßç‡¶ü‡ßá‡¶∞ ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï‡¶ó‡ßÅ‡¶≤‡ßã ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ
                for a in soup.find_all('a', href=True):
                    # ‡¶Æ‡ßÅ‡¶≠‡¶ø‡¶∞ ‡¶®‡¶æ‡¶Æ ‡¶Ø‡¶¶‡¶ø ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï‡ßá ‡¶¨‡¶æ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü‡ßá ‡¶•‡¶æ‡¶ï‡ßá
                    if movie.lower() in a.text.lower() or movie.lower() in a['href'].lower():
                        post_url = a['href']
                        print(f"   üìÇ Opening Post: {post_url}")
                        
                        # ‡¶™‡ßã‡¶∏‡ßç‡¶ü‡ßá‡¶∞ ‡¶≠‡ßá‡¶§‡¶∞ ‡¶ó‡¶ø‡ßü‡ßá ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶ñ‡ßã‡¶Å‡¶ú‡¶æ
                        p_res = session.get(post_url, headers=HEADERS, timeout=20)
                        
                        # ‡¶π‡¶ø‡¶°‡ßá‡¶® ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶ñ‡ßã‡¶Å‡¶ú‡¶æ
                        links = deep_search_links(p_res.text)
                        movie_links.update(links)
                        
                        # ‡¶Ø‡¶¶‡¶ø ‡¶Ü‡¶á‡¶´‡ßç‡¶∞‡ßá‡¶Æ ‡¶•‡¶æ‡¶ï‡ßá
                        p_soup = BeautifulSoup(p_res.text, 'html.parser')
                        for iframe in p_soup.find_all('iframe'):
                            src = iframe.get('src') or iframe.get('data-src')
                            if src and "http" in src:
                                if any(x in src for x in ['dood', 'wish', 'voe', 'player']):
                                    movie_links.add(src)

                time.sleep(2) # ‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶æ‡¶∞‡¶ï‡ßá ‡¶∏‡¶Æ‡ßü ‡¶¶‡¶ø‡¶®
            except Exception as e:
                print(f"   ‚ùå Error: {e}")

        final_data.append({
            "movie": movie,
            "links": list(movie_links),
            "total_found": len(movie_links),
            "last_updated": time.ctime()
        })

    with open('movies.json', 'w', encoding='utf-8') as f:
        json.dump(final_data, f, indent=4, ensure_ascii=False)
    
    print("\n‚úÖ Done! Check movies.json")

if __name__ == "__main__":
    main()
