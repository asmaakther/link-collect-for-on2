import requests
import json
import os
import re
import time
from bs4 import BeautifulSoup
from urllib.parse import urljoin

TARGET_SITES = [
    "https://www.watch-movies.com.pk",
    "https://www.movi.pk"
]

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Referer": "https://www.google.com/"
}

def clean_link(link):
    """‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶•‡ßá‡¶ï‡ßá ‡¶Ö‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶®‡ßÄ‡ßü ‡¶ï‡ßç‡¶Ø‡¶æ‡¶∞‡ßá‡¶ï‡ßç‡¶ü‡¶æ‡¶∞ ‡¶∞‡¶ø‡¶Æ‡ßÅ‡¶≠ ‡¶ï‡¶∞‡ßá ‡¶è‡¶¨‡¶Ç ‡¶™‡ßÇ‡¶∞‡ßç‡¶£‡¶æ‡¶ô‡ßç‡¶ó ‡¶á‡¶â‡¶Ü‡¶∞‡¶è‡¶≤ ‡¶¨‡¶æ‡¶®‡¶æ‡ßü"""
    if not link or "data:image" in link or "base64" in link:
        return None
    link = link.replace('\\', '').strip()
    if link.startswith('//'):
        link = 'https:' + link
    return link

def extract_from_script(html_text):
    """‡¶ú‡¶æ‡¶≠‡¶æ‡¶∏‡ßç‡¶ï‡ßç‡¶∞‡¶ø‡¶™‡ßç‡¶ü ‡¶ï‡ßã‡¶°‡ßá‡¶∞ ‡¶≠‡ßá‡¶§‡¶∞ ‡¶≤‡ßÅ‡¶ï‡¶ø‡ßü‡ßá ‡¶•‡¶æ‡¶ï‡¶æ m3u8 ‡¶¨‡¶æ mp4 ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶ñ‡ßã‡¶Å‡¶ú‡ßá"""
    found = set()
    patterns = [
        r'https?://[^\s"\']+\.m3u8[^\s"\']*',
        r'https?://[^\s"\']+\.mp4[^\s"\']*',
        r'https?://(?:www\.)?(?:doodstream|dood|streamwish|voe|streamtape|fembed)\.[a-z0-9]+/e/[a-zA-Z0-9]+'
    ]
    for pattern in patterns:
        matches = re.findall(pattern, html_text, re.IGNORECASE)
        for m in matches:
            if "ads" not in m.lower():
                found.add(m)
    return found

def main():
    # ‡¶Æ‡ßÅ‡¶≠‡¶ø‡¶∞ ‡¶®‡¶æ‡¶Æ (‡¶Ü‡¶™‡¶®‡¶ø ‡¶ö‡¶æ‡¶á‡¶≤‡ßá ‡¶Ü‡¶∞‡¶ì ‡¶Æ‡ßÅ‡¶≠‡¶ø ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®)
    MOVIE_NAMES = ["Deva", "Dhurandhar", "Avatar"] 
    final_results = []

    for movie in MOVIE_NAMES:
        print(f"\nüîç Searching for: {movie}")
        movie_links = set()

        for site in TARGET_SITES:
            try:
                search_url = f"{site}/?s={movie.replace(' ', '+')}"
                response = requests.get(search_url, headers=HEADERS, timeout=15)
                soup = BeautifulSoup(response.text, 'html.parser')

                # ‡¶∏‡¶æ‡¶∞‡ßç‡¶ö ‡¶∞‡ßá‡¶ú‡¶æ‡¶≤‡ßç‡¶ü ‡¶•‡ßá‡¶ï‡ßá ‡¶™‡ßã‡¶∏‡ßç‡¶ü‡ßá‡¶∞ ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï‡¶ó‡ßÅ‡¶≤‡ßã ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ
                post_links = []
                for a in soup.find_all('a', href=True):
                    if movie.lower() in a.text.lower() or movie.lower() in a['href'].lower():
                        full_post_url = urljoin(site, a['href'])
                        if full_post_url not in post_links:
                            post_links.append(full_post_url)

                # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶Æ‡ßÅ‡¶≠‡¶ø ‡¶™‡ßã‡¶∏‡ßç‡¶ü‡ßá ‡¶¢‡ßÅ‡¶ï‡ßá ‡¶ó‡¶≠‡ßÄ‡¶∞ ‡¶Ö‡¶®‡ßÅ‡¶∏‡¶®‡ßç‡¶ß‡¶æ‡¶®
                for post_url in post_links[:2]: # ‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡ß®‡¶ü‡¶æ ‡¶á‡¶â‡¶®‡¶ø‡¶ï ‡¶™‡ßã‡¶∏‡ßç‡¶ü ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶¨‡ßá
                    print(f"  üìÇ Checking Post: {post_url}")
                    p_res = requests.get(post_url, headers=HEADERS, timeout=15)
                    
                    # ‡ßß. ‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø ‡¶∏‡ßç‡¶ï‡ßç‡¶∞‡¶ø‡¶™‡ßç‡¶ü ‡¶•‡ßá‡¶ï‡ßá ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï ‡¶ñ‡ßã‡¶Å‡¶ú‡¶æ
                    script_links = extract_from_script(p_res.text)
                    movie_links.update(script_links)
                    
                    # ‡ß®. ‡¶Ü‡¶á‡¶´‡ßç‡¶∞‡ßá‡¶Æ ‡¶ì ‡¶è‡¶Æ‡¶¨‡ßá‡¶° ‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó ‡¶•‡ßá‡¶ï‡ßá ‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶ñ‡ßã‡¶Å‡¶ú‡¶æ
                    p_soup = BeautifulSoup(p_res.text, 'html.parser')
                    for tag in p_soup.find_all(['iframe', 'embed', 'source', 'video']):
                        # ‡¶∏‡¶¨ ‡¶ß‡¶∞‡¶£‡ßá‡¶∞ ‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶ü‡ßç‡¶∞‡¶ø‡¶¨‡¶ø‡¶â‡¶ü ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶æ (Lazy loading bypass)
                        potential_src = (
                            tag.get('src') or 
                            tag.get('data-src') or 
                            tag.get('data-lazy-src') or 
                            tag.get('data-litesrc') or
                            tag.get('data-original')
                        )
                        
                        valid_link = clean_link(potential_src)
                        if valid_link:
                            # ‡¶Ø‡¶¶‡¶ø ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï‡¶ü‡¶ø ‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø ‡¶Æ‡ßÅ‡¶≠‡¶ø ‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶æ‡¶∞ ‡¶π‡ßü
                            if any(x in valid_link for x in ['.m3u8', '.mp4', 'dood', 'streamwish', 'voe', 'player']):
                                movie_links.add(valid_link)
                                print(f"    ‚úÖ Found: {valid_link[:60]}...")
                
                time.sleep(1)
            except Exception as e:
                print(f"  ‚ùå Error on {site}: {e}")

        final_results.append({
            "movie": movie,
            "links": list(movie_links),
            "total_links_found": len(movie_links),
            "last_updated": time.ctime()
        })

    # JSON ‡¶∏‡ßá‡¶≠ ‡¶ï‡¶∞‡¶æ
    with open('movies.json', 'w', encoding='utf-8') as f:
        json.dump(final_results, f, indent=4, ensure_ascii=False)
    
    print("\n‚úÖ Scraping Completed! Results saved in movies.json")

if __name__ == "__main__":
    main()
